[{"authors":["admin"],"categories":null,"content":"He Zhang is a Postdoctoral Associate at VCU Robotics Lab. His research interests include SLAM, robotics vision, indoor localization, 3D mapping, human pose tracking, and machine learning. He endeavors to build robust intelligent systems to assist blind people for navigation (CRC, W-ROMA) and mobility-impaired patients for rehabilitation (Q-HARP).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rising-turtle.github.io/david_zhang/author/he-zhang-david/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/david_zhang/author/he-zhang-david/","section":"authors","summary":"He Zhang is a Postdoctoral Associate at VCU Robotics Lab. His research interests include SLAM, robotics vision, indoor localization, 3D mapping, human pose tracking, and machine learning. He endeavors to build robust intelligent systems to assist blind people for navigation (CRC, W-ROMA) and mobility-impaired patients for rehabilitation (Q-HARP).","tags":null,"title":"He Zhang (David)","type":"authors"},{"authors":["He Zhang","Cang Ye"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"9dcd687341007e36372e91770b198a23","permalink":"https://rising-turtle.github.io/david_zhang/publication/iros-20-1/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/iros-20-1/","section":"publication","summary":"This paper presents a new RGB-D-camera-based visual-inertial odometry (VIO), termed DUI-VIO, for estimating the motion state of the camera. First, a Gaussian mixture model (GMM) to is employed to model the uncertainty of the depth data for each pixel on the camera’s color image. Second, the uncertainties are incorporated into the VIO’s initialization and optimization processes to make the state estimate more accurate. In order to perform the initialization process, we propose a hybrid-perspective-n-point (PnP) method to compute the pose change between two camera frames and use the result to triangulate the depth for an initial set of visual features whose depth values are unavailable from the camera. Hybrid-PnP first uses a 2D-2D PnP algorithm to compute rotation so that more visual features may be used to obtain a more accurate rotation estimate. It then uses a 3D-2D scheme to compute translation by taking into account the uncertainties of depth data, resulting in a more accurate translation estimate. The more accurate pose change estimated by Hybrid-PnP help to improve the initialization result and thus the VIO performance in state estimation. In addition, Hybrid-PnP make it possible to compute the pose change by using a small number of features with a known depth. This improves the reliability of the initialization process. Finally, DUI-VIO incorporates the uncertainties of the inverse depth measurements into the nonlinear optimization process, leading to a reduced state estimation error. Experimental results validate that the proposed DUI-VIO method outperforms the state-of-the-art VIO methods in terms of accuracy and reliability.","tags":null,"title":"DUI-VIO: Depth Uncertainty Incorporated Visual Inertial Odometry based on an RGB-D Camera","type":"publication"},{"authors":["He Zhang","Lingqiu Jin","Cang Ye"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"b56ac6d89ab976d5021bc25b03941676","permalink":"https://rising-turtle.github.io/david_zhang/publication/iros-20-2/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/iros-20-2/","section":"publication","summary":"This paper presents VCU-RVI, a new visual inertial odometry (VIO) benchmark with a set of diverse data sequences in different indoor scenarios. The benchmark is captured using an Structure Core (SC) sensor, consisting of RGB-D camera and an IMU. It provides aligned color and depth images with 640x480 resolution at 30 Hz. The camera's data is synchronized with the IMU's data at 100 Hz. Thirty-nine data sequences covering a total of ~3.7 kilometers trajectory are recorded in various indoor environments by two experimental setups$:$ holding it by a hand or installing it on a wheeled robot. For the handheld SC data sequences in the laboratory, they are recorded under three challenging conditions$:$ fast motion, radical illumination changing, and dynamic objects. Besides, data sequences for long distance indoor navigation are collected covering different indoor scenarios$:$ room, corridor, hall, and stairway. For the data sequences captured using the wheeled robot, half of them are recorded with sufficient IMU excitation in the beginning of the sequence, allowing to test the VIO methods with the requirement of sufficient motion conditions for initialization. We also put three bumpers in the laboratory to simulate bumpy road scenarios where the robot's motion is of 6-DOF. In addition, data sequences for long distance travel are also collected by the wheel robot. For trajectory evaluation, we use a motion capture system (120 Hz) to provide accurate pose ground truth. We conduct experiments to evaluate state-of-the-art VIO algorithms using our benchmark. The VCU-RVI dataset, the evaluated VIO methods, and the evaluation tools to generate the experimental results are made public.","tags":null,"title":"The VCU-RVI Benchmark: Evaluating Visual Inertial Odometry for Indoor Navigation Applications with an RGB-D Camera","type":"publication"},{"authors":["He Zhang","Cang Ye"],"categories":null,"content":"","date":1600560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600560000,"objectID":"6d7091f10731414d0afa475134925c18","permalink":"https://rising-turtle.github.io/david_zhang/publication/icra-20/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/icra-20/","section":"publication","summary":"This paper presents a new visual positioning system (VPS) for real-time pose estimation of a robotic navigation aid (RNA) that is used for assistive navigation of a visually impaired person. The backbone of the VPS is a new depth-enhanced visual-inertial odometry (DVIO) that uses an RGB-D camera and an inertial measurement unit (IMU) to estimate the 6-DOF pose of the device. The DVIO method extracts the geometric feature (the floor plane in this work) from the camera's depth data and integrates its measurement residuals with that of the visual features and the inertial data in a graph optimization framework for pose estimation. A new measure based on the Sampson error is introduced to describe the measurement residuals of the near-range visual features whose depths (measured by the depth camera) are accurate and that of the far-range visual features whose depths are unknown. The measure allows for the incorporation of both types of visual features into the graph optimization. The use of the geometric feature and the Sampson error improve pose estimation accuracy and precision. The DVIO method is paired with a particle-filter-based localization (PFL) method to localize the RNA in a 2D floor plan and use the information to guide the visually impaired person to reach the destination. The PFL reduces the RNA's position and heading error on the horizontal plane by aligning the camera's depth data with the floor plan map. Together, the DVIO and the PFL allow the VPS to accurately estimate the pose of the RNA for wayfinding and meanwhile generate a 3D local map for obstacle avoidance. Experimental results demonstrate the usefulness of the RNA in wayfinding and obstacle avoidance for the visually impaired in indoor spaces.","tags":null,"title":"A Visual Positioning System for Indoor Blind Navigation","type":"publication"},{"authors":["Lingqiu Jin","He Zhang","Cang Ye"],"categories":null,"content":"","date":1590364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590364800,"objectID":"a16f5a4029b949dd9e157ff02f9c1c73","permalink":"https://rising-turtle.github.io/david_zhang/publication/tmech-20/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/tmech-20/","section":"publication","summary":"The increasing computing and sensing capabilities of modern mobile phones have spurred research interests in developing new visual-inertial odometry (VIO) techniques to turn a smartphone into a self-contained vision-aided inertial navigation system for various applications. Smartphones nowadays use cameras with optical image stabilization (OIS) technology to reduce image blurs. When in action, the mechanism results in varying camera intrinsic parameters (CIP), which must be taken into account in the VIO computation. In this paper, we first develop a linear model to relate the CIP with the IMU-measured acceleration. Based on the model, we introduce a new VIO method, called CIP-VMobile, which treats CIP as state variables and tightly couples them with other state variables in a graph optimization process to estimate the optimal state. The method uses the linear model to construct a factor graph and uses the linear-model-computed values as initial CIP estimates to speed up the VIO computation and attain a better pose estimation result. Simulation and experimental results with an iPhone7 validate the method's efficacy. Based on CIP-VMobile, we fabricated a robotic navigation aid based on an iPhone 7 for assisted navigation. Experimental results with the RNA demonstrate CIP-VMobile's promise in real-world navigation applications.","tags":null,"title":"Camera Intrinsic Parameters Estimation by Visual Inertial Odometry for a Mobile Phone with Application to Assisted Navigation","type":"publication"},{"authors":["He Zhang","Cang Ye"],"categories":null,"content":"","date":1590364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590364800,"objectID":"dac69e68c7e35e85ab590672a1b594da","permalink":"https://rising-turtle.github.io/david_zhang/publication/access-20/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/access-20/","section":"publication","summary":"The classic visual-inertial odometry (VIO) method estimates the 6-DOF pose of a moving camera by fusing the camera’s ego-motion estimated by visual odometry (VO) and the motion measured by an inertial measurement unit (IMU). The VIO attempts to updates the estimates of the IMU’s biases at each step by using the VO’s output to improve the accuracy of IMU measurement. This approach works only if an accurate VO output can be identified and used. However, there is no reliable method that can be used to perform an online evaluation of the accuracy of the VO. In this paper, a new VIO method is introduced for pose estimation of a robotic navigation aid (RNA) that uses a 3D time-of-flight camera for assistive navigation. The method, called plane-aided visual-inertial odometry (PAVIO), extracts planes from the 3D point cloud of the current camera view and track them onto the next camera view by using the IMU’s measurement. The covariance matrix of each tracked plane’s parameters is computed and used to perform a plane consistent check based on a chi-square test to evaluate the accuracy of VO’s output. PAVIO accepts a VO output only if it is accurate. The accepted VO outputs, the information of the extracted planes, and the IMU’s measurements over time are used to create a factor graph. By optimizing the graph, the method improves the accuracy in estimating the IMU bias and reduces the camera’s pose error. Experimental results with the RNA validate the effectiveness of the proposed method. PAVIO can be used to estimate the 6-DOF pose for any 3D-camera-based visual-inertial navigation system.","tags":null,"title":"Plane-Aided Visual-Inertial Odometry for 6-DOF Pose Estimation of a Robotic Navigation Aid","type":"publication"},{"authors":["Lingqiu Jin","He Zhang","Cang Ye"],"categories":null,"content":"","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"3c5f850b72662781c22c028b39e46c99","permalink":"https://rising-turtle.github.io/david_zhang/publication/ichms-20/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/ichms-20/","section":"publication","summary":"This paper presents a new hand-worn device, called wearable robotic object manipulation aid (W-ROMA), that can help a visually impaired individual locate a target object and guide the hand to take a hold of it. The device consists of a sensing unit and a guiding unit. The sensing unit uses a Structure Core sensor, comprising of an RGB-D camera and an Inertial Measurement Unit (IMU), to detect the target object and estimate the device pose. Based on the object and pose information, the guiding unit computes the Desired Hand Movement (DHM) and convey it to the user by an electro-tactile display to guide the hand to approach the object. A speech interface is developed and used as an additional way to convey the DHM and used for human-robot interaction. A new method, called Depth Enhanced Visual-Inertial Odometry (DVIO), is proposed for 6-DOF device pose estimation. It tightly couples the camera’s depth and visual data with the IMU data in a graph optimization process to produce more accurate pose estimation than the existing state-of-the-art approach. Experimental results demonstrate that the DVIO method outperforms the state-of-the-art VIO approach in 6-DOF pose estimation.","tags":null,"title":"Human-Robot Interaction for Assisted Object Grasping by a Wearable Robotic Object Manipulation Aid for the Blind","type":"publication"},{"authors":["He Zhang","Lingqiu Jin","Cang Ye"],"categories":null,"content":"","date":1574208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574208000,"objectID":"6da7d780a621b23e3b3b23862a467c5c","permalink":"https://rising-turtle.github.io/david_zhang/publication/iros-19-w/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/iros-19-w/","section":"publication","summary":"This paper presents a new method, called depth-enhanced visual-inertial odometry (DVIO), for real-time pose estimation of a robotic navigation aid (RNA) for assistive wayfinding. The method estimates the device pose by using an RGB-D camera and an inertial measurement unit (IMU). It extracts the floor plane from the camera's depth data and tightly couple the floor plane, the visual features (with depth data from the RGB-D camera or unknown depth), and the IMU's inertial data in a graph optimization framework for 6-DOF pose estimation. Due to use of the floor plane and the depth data from the RGB-D camera, the DVIO method has a better pose estimation accuracy than its VIO counterpart. To enable real-time computing on the RNA, the size of the sliding window for the graph optimization is reduced to trade some accuracy for computational efficiency. Experimental results demonstrate that the method achieved a pose estimation accuracy similar to that of the state of the art VIO but ran at a much faster speed (with a pose update rate of 18 Hz).","tags":null,"title":"A Depth-Enhanced Visual Inertial Odometry for a Robotic Navigation Aid for Blind People","type":"publication"},{"authors":["He Zhang","Cang Ye"],"categories":null,"content":"","date":1560988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560988800,"objectID":"a45a990506bc4374f21b919c715cec71","permalink":"https://rising-turtle.github.io/david_zhang/publication/hsi-19/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/hsi-19/","section":"publication","summary":"This paper introduces a new robotic navigation aid (RNA) for the visually impaired (VI). Two fundamental functions wayfinding and human-robot interaction (HRI) are presented for assisted wayfinding. The problem of wayfinding involves planning a path from the RNA’s current location to the destination and following the path to get to the destination. To address the problem, we developed a new visual inertial odometry to estimate the RNA’s pose by using the image and depth data from an RGB-D camera and the inertial data of an IMU. The estimated pose is used for path planning. To guide the user to follow the planned path, we designed an HRI interface with two guiding modes the robocane mode and white-came mode. In the robocane mode, the RNA uses a motorized rolling tip to steer itself into the desired direction of travel (DDT) for the user to follow and track the planned path. In the white-cane mode, the RNA uses its speech interface to indicate the DDT to the user by audio messages. In this mode, the user swings the RNA just like using a conventional white cane. To make mode selection effortless, we developed a human intent detection (HID) method based on the decision tree mode. The method can detect the user intent and automatically select the appropriate mode according to the detected intent. Experimental results demonstrate the efficacies of the VIO, HRI, and HID methods for assisted wayfinding.","tags":null,"title":"Human-Robot Interaction for Assisted Wayfinding of a Robotic Navigation Aid for the Blind","type":"publication"},{"authors":["He Zhang","Lingqiu Jin","Hao Zhang","Cang Ye"],"categories":null,"content":"","date":1547942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547942400,"objectID":"ca0e2c4d4164f5076c1fb9daa8b6d34a","permalink":"https://rising-turtle.github.io/david_zhang/publication/wacv-19/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/wacv-19/","section":"publication","summary":"This paper compares the performance of three state-of-the-art visual-inertial simultaneous localization and mapping (SLAM) methods in the context of assisted wayfinding of the visually impaired. Specifically, we analyze their strengths and weaknesses for assisted wayfinding of a robotic navigation aid (RNA). Based on the analysis, we select the best visual-inertial SLAM method for the RNA application and extend the method by integrating with it a method capable of detecting loops caused by the RNA’s unique motion pattern. By incorporating the loop closures in the graph and optimization process, the extended visual-inertial SLAM method reduces the pose estimation error. The experimental results with our own datasets and the TUM VI benchmark datasets confirm the advantage of the selected method over the other two and validate the efficacy of the extended method.","tags":null,"title":"A Comparative Analysis of Visual-Inertial SLAM for Assisted Wayfinding of the Visually Impaired","type":"publication"},{"authors":["Xiaoping Liu","He Zhang","Lingqiu Jin","Cang Ye"],"categories":null,"content":"","date":1545264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545264000,"objectID":"d84a01d20caf5647e3ccaf1b3514df37","permalink":"https://rising-turtle.github.io/david_zhang/publication/nsens-18/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/nsens-18/","section":"publication","summary":"Due to loss of vision, it becomes a challenging task for a visually impaired individual to locate and grasp a target object. This paper presents a hand-worn assistive device that may assist a visually impaired person in detecting a target object and maintaining alignment with the object while approaching it. The device consists of a sensing module and a guiding module. The sensing module uses an RGB-D camera to detect and track the target object. The guiding module computes hand-object misalignment, determines the desired hand movement (DHM) for hand-object alignment, and uses a cable-driven exoskeleton mechanism to guide the user’s hand to align with the target object. The guiding module helps to maintain hand-object alignment while the hand is approaching the object. A prototype of the device was developed and its usability was validated by experiments with human subjects.","tags":null,"title":"A Wearable Robotic Object Manipulation Aid for the Visually Impaired","type":"publication"},{"authors":["Md. Rayhan Afsar","Michael Wadsworth","Tao Shen","He Zhang","Cang Ye","Xiangrong Shen"],"categories":null,"content":"","date":1537401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537401600,"objectID":"512f4ed534dfd34a1c3ea2ba7c8a3b43","permalink":"https://rising-turtle.github.io/david_zhang/publication/dmd-17/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/dmd-17/","section":"publication","summary":"","tags":null,"title":"A Motorized Robotic Walker for Human Walking Assistance","type":"publication"},{"authors":["Tao Shen","Md. Rayhan Afsar","He Zhang","Cang Ye","Xiangrong Shen"],"categories":null,"content":"","date":1537401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537401600,"objectID":"44d19dca0d26dca002776c2c5a300c01","permalink":"https://rising-turtle.github.io/david_zhang/publication/dsc-18/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/dsc-18/","section":"publication","summary":"Impaired mobility negatively impacts a patient’s life, both physically and mentally. Existing tools for the assistance of such individuals include passive walking aids, which tend to disrupt the users’ walking rhythms, and motorized wheelchairs, which preclude the muscle activity of the users’ lower limb. To address these problems, this paper proposes a new robotic walker guided by an image processing system. This system uses visual information from a 3D camera to estimate the user’s position and orientation, and the motion are sent to a PID controller to actuate the robot so that the robot can automatically follow the user to provide assistance if needed. Therefore, for regular walking, the device can accompany the user without interfering with the muscle activity of the lower limbs; for rehabilitation training, it may support the user and provide protection. To reduce the errors of the estimated position and orientation, a discrete derivative term is added into a conventional PI controller to form a mixed PID controller which can reduce the noise from measurement or image processing. Experimental results demonstrated the feasibility of the proposed robotic method, as well as the functionality and effectiveness of the whole robotic system.","tags":null,"title":"Development of a Motorized Robotic Walker Guided by an Image Processing System for Human Walking Assistance and Rehabilitation","type":"publication"},{"authors":["He Zhang","Cang Ye"],"categories":null,"content":"","date":1505865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505865600,"objectID":"4934f3e92d9790838ff5e279405b4811","permalink":"https://rising-turtle.github.io/david_zhang/publication/bmvc-17/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/bmvc-17/","section":"publication","summary":"The classic visual-inertial odometry (VIO) method estimates a moving camera’s 6-DOF pose relative to its starting point by fusing the camera’s ego-motion measured by a visual odometry (VO) and the motion measured by an inertial measurement unit (IMU). The VIO attempts to updates the estimates of the IMU’s biases at each step by using the VO’s output so as to improve the accuracy of IMU measurement. This approach works only if an accurate VO output can be identified and used. However, there is no reliable method that can be used to evaluate the accuracy of the VO. In this paper, a new VIO method is introduced for pose estimation of a robotic navigation aid (RNA) that uses a 3D time-of-flight camera for perception. The method, called plane-aided visual-inertial odometry (PAVIO), extracts planes from the 3D point cloud of the current camera view and track them onto the next camera view by using the IMU’s measurement. The tracking result is used to accept the VO output only if it is accurate. The accepted VO outputs, the information of the extracted planes, and the IMU’s measurements over time are used to create a factor graph. By optimizing the graph, the method improves the estimation accuracy of the IMU bias and reduces the camera’s pose error. Experimental results with the RNA validate the effectiveness of the proposed method.","tags":null,"title":"Plane-Aided Visual-Inertial Odometry for Pose Estimation of a 3D Camera based Indoor Blind Navigation System","type":"publication"},{"authors":["He Zhang","Cang Ye"],"categories":null,"content":"","date":1490400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490400000,"objectID":"51ffa0db0e01bd97a7110303e02ecbbf","permalink":"https://rising-turtle.github.io/david_zhang/publication/tnsre-17/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/tnsre-17/","section":"publication","summary":"This paper presents a 6-DOF pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired. The PE method involves two graph SLAM processes to reduce the accumulative pose error of the device. In the first step, the floor plane is extracted from the 3D camera’s point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch and Z errors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce X, Y and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a visually impaired person in an indoor environment. The system uses the estimated pose and floorplan to locate the device user in a building and guides the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task.","tags":null,"title":"An Indoor Wayfinding System Based on Geometric Features Aided Graph SLAM for the Visually Impaired","type":"publication"},{"authors":["He Zhang","Cang Ye"],"categories":null,"content":"","date":1485302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485302400,"objectID":"6b961ccab3b08707517ad8c95b93477c","permalink":"https://rising-turtle.github.io/david_zhang/publication/ijira-17/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/ijira-17/","section":"publication","summary":"This paper presents a walking pattern detection method for a smart rollator. The method detects the rollator user’s lower extremities from the depth data of an RGB-D camera. It then segments the 3D point data of the lower extremities into the leg and foot data points, from which a skeletal system with 6 skeletal points and 4 rods is extracted and used to represent a walking gait. A gait feature, comprising the parameters of the gait shape and gait motion, is then constructed to describe a walking state. K-means clustering is employed to cluster all gait features obtained from a number of walking videos into 6 key gait features. Using these key gait features, a walking video sequence is modeled as a Markov chain. The stationary distribution of the Markov chain represents the walking pattern. Three Support Vector Machines (SVMs) are trained for walking pattern detection. Each SVM detects one of the three walking patterns. Experimental results demonstrate that the proposed method has a better performance in detecting walking patterns than seven existing methods.","tags":null,"title":"RGB-D Camera Based Walking Pattern Recognition by Support Vector Machines for a Smart Rollator","type":"publication"},{"authors":["He Zhang","Cang Ye"],"categories":null,"content":"","date":1482192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482192000,"objectID":"cbdd74feb5f971ce7b38daa44b217fbd","permalink":"https://rising-turtle.github.io/david_zhang/publication/robio-16/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/robio-16/","section":"publication","summary":"This paper presents a 6-DOF pose estimation (PE) method and an indoor wayfinding system based on the method for the visually impaired (VI). The PE method involves two graph SLAM processes to reduce the accumulative pose error of device. In the first step, the floor plane is extracted from the 3D camera’s point cloud and added as a landmark node into the graph for 6-DOF SLAM to reduce roll, pitch and Z errors. In the second step, the wall lines are extracted and incorporated into the graph for 3-DOF SLAM to reduce X, Y and yaw errors. The method reduces the 6-DOF pose error and results in more accurate pose with less computational time than the existing state-of-the-art planar SLAM methods. Based on the PE method, a wayfinding system is developed for navigating a VI person in an indoor environment. The system uses the estimated pose and floorplan to locate the device user in a building and guide the user by announcing the points of interest and navigational commands through a speech interface. Experimental results validate the effectiveness of the PE method and demonstrate that the system may substantially ease an indoor navigation task.","tags":null,"title":"An indoor navigation aid for the visually impaired","type":"publication"},{"authors":["He Zhang","Cang Ye"],"categories":null,"content":"","date":1482192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482192000,"objectID":"517917d529e408e87fb5898fbc74b16b","permalink":"https://rising-turtle.github.io/david_zhang/publication/isvc-15/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/isvc-15/","section":"publication","summary":"This paper presents a walking pattern detection method for a smart rollator. The method detects the rollator user's lower extremities from the depth data of an RGB-D camera. It then segments the 3D point data of the lower extremities into the leg and foot data points, from which a skeletal system with 6 skeletal points and 4 rods is extracted and used to represent a walking gait. A gait feature, comprising the parameters of the gait shape and gait motion, is then constructed to describe a walking state. K-means clustering is employed to cluster all gait features obtained from a number of walking videos into 6 key gait features. Using these key gait features, a walking video sequence is modeled as a Markov chain. The stationary distribution of the Markov chain represents the walking pattern. Five Support Vector Machines (SVMs) are trained for walking pattern detection. Each SVM detects one of the five walking patterns. Experimental results demonstrate that the proposed method has a better performance in detecting walking patterns than three existing methods.","tags":null,"title":"An RGB-D Camera based Walking Pattern Detection Method for Smart Rollators","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"558ad7f2b7b11c5eb9600e4cb2685de1","permalink":"https://rising-turtle.github.io/david_zhang/project/crc/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/david_zhang/project/crc/","section":"project","summary":"A CRC to aid a visually impaired person for wayfinding and obstacle avoidance.","tags":["CRC"],"title":"Co-Robotic Cane (CRC)","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"3bd012e7acd146a7b59085d6fc35183e","permalink":"https://rising-turtle.github.io/david_zhang/project/w-roma/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/david_zhang/project/w-roma/","section":"project","summary":"A Wearable Robotic Object Manipulation Aid (W-ROMA) to assist a visually impaired individual to locate and manipulate a target object.","tags":["W-ROMA"],"title":"W-ROMA","type":"project"},{"authors":null,"categories":null,"content":"The proposed project aims at developing a novel quadrupedal human-assistive robotic platform (Q-HARP), with the objective of helping the large number of elderly individuals live a healthy, physically active life style. The Q-HARP primarily functions as a smart power-assist walker: a user, situated in the center of the legged robot, can enjoy the desired amount of power assist in walking according to his/her intent. As such, a Q-HARP user is able to perform an appropriate amount of physical activity, while enjoying the enhanced level of mobility during his/her daily lif. Furthermore, with the Q-HARP\u0026rsquo;s legged motion, the human-robot system can easily overcome most obstacles in the daily life. With this capability, the Q-HARP has a potential of becoming a basic building block for Aging-in-Place, enabling elderly individuals to stay in their home longer without conducting costly modification to their existing home structures.\n","date":1440547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440547200,"objectID":"23850ea1b98b0f4a45976d6728548bf4","permalink":"https://rising-turtle.github.io/david_zhang/project/q-harp/","publishdate":"2015-08-26T00:00:00Z","relpermalink":"/david_zhang/project/q-harp/","section":"project","summary":"A quadrupedal human-assistive robotic platform (Q-HARP) to aid the elderly for safe walking.","tags":null,"title":"Q-HARP","type":"project"},{"authors":["He Zhang","Guoliang Liu","Zifeng Hou"],"categories":null,"content":"","date":1411171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1411171200,"objectID":"d65ebef86cd05b111c57198612109677","permalink":"https://rising-turtle.github.io/david_zhang/publication/iros-14-w/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/iros-14-w/","section":"publication","summary":"The-state-of-the-art graph optimization method can robustly converge into a solution with least square errors for the graph structure. Nevertheless, when a biased edge (erroneous transformation with over-confident information matrix) exists, the optimal solution can produce the large deviation because of error propagation produced by the biased edge. In order to solve this problem in graph-based 2D SLAM system, this paper proposed an iterative graph optimization approach. To reduce the errors propagated from the biased edges, we iteratively reconstruct the graph structure by referring to the result of the graph optimization process. Meanwhile, to maintain the information of the other well estimated edges, we strictly update the graph structure by considering the scan-correlation score and the marginal covariance. In addition, we apply a novel key-node mechanism to robustly detect the loop-closure by a linear interpolation algorithm. The experiments show that the proposed method is more robust and accurate than the previous methods when the biased edges exist.","tags":null,"title":"An Iterative Graph Optimization Approach for 2D SLAM","type":"publication"},{"authors":["He Zhang","Zifeng Hou","Nanjun Li","Shuang Song","Pengzhi Xu"],"categories":null,"content":"","date":1355961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1355961600,"objectID":"4d2a442bfc54daf7380ae3f2cfb6e9db","permalink":"https://rising-turtle.github.io/david_zhang/publication/icarcv-12/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/icarcv-12/","section":"publication","summary":"In this paper, we present a duo-graph Simultaneous Localization and Mapping (SLAM) method that enables visual-guided robot to efficiently and robustly generate consistent 3D map in a large-scale environment. Recently the conditional independent graph (CI-GRAPH) has been proved to be an efficient and robust method for solving the large-scale SLAM problem. However, it stems from extended kalman filter (EKF) SLAM that suffers from high computational load when number of the landmarks becomes large. Moreover, the big noise in the measurement can lead EKF SLAM far from convergence. These disadvantages are the cases in the visual-guided robot. To solve these problems, we propose a duo-graph structure that sustains the feasibility of a hierarchical graph-based SLAM framework. It implements two level graph-based SLAM$:$ local and global SLAM. Utilizing the characteristic of the duo-graph structure, the local SLAM can efficiently localize itself while the global SLAM can accurately close loops in the large scale environment. In addition, our method can filter massive noisy visual features and eliminate mismatches in the global SLAM process. To demonstrate the superiority of our method, both simulation and real experiments are carried out.","tags":null,"title":"Duo-graph: An efficient and robust method for large-scale mapping for visual-guided robots","type":"publication"},{"authors":["He Zhang","Zifeng Hou","Nanjun Li","Shuang Song"],"categories":null,"content":"","date":1350691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1350691200,"objectID":"66594034076598780679a5d88993fd4f","permalink":"https://rising-turtle.github.io/david_zhang/publication/icira-12/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/david_zhang/publication/icira-12/","section":"publication","summary":"In this paper, a graph-based hierarchical SLAM framework is proposed which ensures not only the high-speed operation of graph-based SLAM, but also feasibility of large-scale 3D map building. Both local and global level graph-based SLAM will be operated. They can be concurrently implemented in different computing units but must maintained communication which is called as a session. During each session, local level SLAM will create a pose-graph containing trajectory for mobile robot and information for local maps. To avoid communication congestion, raw density point cloud of each pose-node will be reduced into a sparser one by voxel grid filtering. When a session is closed, duo-graph strategy is executed to guarantee the consistency between successive local map. To associate massive local map information and closing loops in large-scale environment, graph-based algorithm will also be implemented in global level SLAM. Two experiments are carried out in the real indoor environment. In the first experiment, SLAM process is greatly accelerated in this framework by distributing the whole SLAM task into different level SLAM entities$:$ local level SLAM operated on robot and global on laptop. It shows that this framework is scalable and it can be implemented in CS(Client-Server) model. Second experiment is conducted in our biggest work office around which we control the robot traverse for three times. It demonstrates that this framework can simultaneously keep fast graph-based SLAM in local-end and generate consistent and convergent 3D map in global-end SLAM process.","tags":null,"title":"A Graph-Based Hierarchical SLAM Framework for Large-Scale Mapping","type":"publication"}]